{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09f4776f",
   "metadata": {},
   "source": [
    "<h4> Datasets and Resources </h4> \n",
    "\n",
    "* WikiText-2 (raw/unprocessed), Train, Dev, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ca6d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "07c70eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLanguageModel:\n",
    "    def __init__(self) -> None:\n",
    "        self.tokens = []\n",
    "        self.word_frequency = {}\n",
    "        self.bigrams = {}\n",
    "        self.predictions = []\n",
    "\n",
    "    def tokenize(self,data):\n",
    "        '''\n",
    "        Data preprocessing\n",
    "        '''\n",
    "        words = re.findall(r'\\b[A-Za-z]+\\b', data.lower())\n",
    "        return words\n",
    "\n",
    "    def train(self, text):\n",
    "        '''\n",
    "        Train the model by:\n",
    "        - Tokenization\n",
    "        - Getting the frequency of each word\n",
    "        - Getting the frequency of each pair (Bigrams)\n",
    "        '''\n",
    "        # Tokenization\n",
    "        words = self.tokenize(text)\n",
    "        self.tokens = words\n",
    "\n",
    "        # Word frequency\n",
    "        for i in self.tokens:\n",
    "            if i in self.word_frequency.keys():\n",
    "                self.word_frequency[i] += 1\n",
    "            else:\n",
    "                self.word_frequency.update({i:1})\n",
    "\n",
    "        # Bigram creation\n",
    "        for i in range(len(words)-1):\n",
    "            pair = (words[i],words[i+1])\n",
    "            if pair not in self.bigrams:\n",
    "                self.bigrams.update({pair:1})\n",
    "            else:\n",
    "                self.bigrams[pair] += 1\n",
    "            \n",
    "        # Sorting the dictionary by value (frequency) so that when i predict the next word it should find the searched word with the highest frequency\n",
    "        self.bigrams = sorted(self.bigrams.items(), key=lambda x: x[1], reverse=True)\n",
    "        self.bigrams = dict(self.bigrams)\n",
    "\n",
    "        print('Model Trained Successfully!')\n",
    "\n",
    "    def predict_next_word(self,start_word:str):\n",
    "        '''\n",
    "        The prediction of the most frequent next word\n",
    "        '''\n",
    "        max_freq = 0\n",
    "        next_word = None\n",
    "        for bigr, freq in self.bigrams.items():\n",
    "            if bigr[0] == start_word and freq > max_freq:\n",
    "                max_freq = freq\n",
    "                next_word = bigr[1]\n",
    "\n",
    "        return next_word\n",
    "    \n",
    "    def predict(self,epochs,start_word:str):\n",
    "        '''\n",
    "        Prediction of the whole sequence with 'epochs' length\n",
    "        '''\n",
    "        if ' ' in start_word:\n",
    "            test_text = self.tokenize(start_word)\n",
    "            start_word = test_text[0]\n",
    "            \n",
    "        if epochs > 0:\n",
    "            nextW = self.predict_next_word(start_word)\n",
    "            self.predictions.append(nextW)\n",
    "            print(nextW)\n",
    "            return self.predict(epochs=epochs-1,start_word=nextW)\n",
    "        else:\n",
    "            print(f'\\n The prediction is done')\n",
    "\n",
    "    def error(self):\n",
    "        '''\n",
    "        Error between the testing text and our predictions\n",
    "        A better estimator is perplexity\n",
    "        '''\n",
    "        if self.tokens == []:\n",
    "            print('Please train the model, then make some predictions to be able to get the error!')\n",
    "\n",
    "        if self.predictions != []:\n",
    "            tr_len = len(self.tokens)\n",
    "            min_len = min(len(self.tokens),len(self.predictions))\n",
    "            counter = 0\n",
    "            for i in range(min_len):\n",
    "                if self.tokens[i] == self.predictions[i]:\n",
    "                    counter += 1\n",
    "\n",
    "            error = counter / tr_len\n",
    "            return error\n",
    "        else:\n",
    "            print('You need to predict first!')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "014216a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('PATH','r', encoding='utf-8') as train_file:\n",
    "    train_data = train_file.read()\n",
    "\n",
    "with open('PATH','r', encoding='utf-8') as test_file:\n",
    "    test_data = test_file.read()\n",
    "\n",
    "with open('PATH','r', encoding='utf-8') as valid_file:\n",
    "    valid_data = valid_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1d7b13c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Trained Successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create the model and train the data\n",
    "\n",
    "lmO = MyLanguageModel()\n",
    "lmO.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "683fd644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "le\n",
      "soir\n",
      "jeunesse\n",
      "was\n",
      "the\n",
      "first\n",
      "time\n",
      "the\n",
      "first\n",
      "time\n",
      "the\n",
      "first\n",
      "time\n",
      "the\n",
      "first\n",
      "time\n",
      "the\n",
      "first\n",
      "time\n",
      "the\n",
      "first\n",
      "time\n",
      "the\n",
      "first\n",
      "time\n",
      "the\n",
      "first\n",
      "time\n",
      "the\n",
      "first\n",
      "time\n",
      "the\n",
      "first\n",
      "time\n",
      "the\n",
      "first\n",
      "time\n",
      "the\n",
      "first\n",
      "time\n",
      "the\n",
      "first\n",
      "time\n",
      "the\n",
      "first\n",
      "time\n",
      "the\n",
      "first\n",
      "time\n",
      "the\n",
      "\n",
      " The prediction is done\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "\n",
    "lmO.predict(50,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "445ce201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.953600022861824e-07"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how good our predictions are. [Hint: they are much better at around 1.0 and really bad at around 0.0\n",
    "\n",
    "lmO.error()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
